# ОТЧЕТ ПО ДИСЦИПЛИНЕ : ОБУЧЕНИЕ С ПОДКРЕПЛЕНИЕМ И НЕЙРОННЫЕ СЕТИ""

## ПОСТАНОВКА ЗАДАЧИ
Для введения в курс дела, кратко изложим проблему, которую необходимо решить.
**Mountain car problem** - задача, которая представляет собой одну из стандартных **сред** (областей) тестирования в обучении с подкреплением. Основная суть заключается в следующем: маломощному автомобилю необходимо забраться на гору и достичь некоторой условной цели, для этого ему нужно преодолеть достаточно крутой склон. Поскольку гравитация сильнее, чем двигатель автомобиля, даже на полном газу, невозможно просто ускориться вверх по крутому склону и достичь "флага". При этом автомобиль находится в некотором "ущелье" и должен научиться использоваться потенциальную энергию **Ep**, поднимаясь на противоположный склон. Тем самым, автомобиль переведет часть своей потенциальной энергии **Ep** в кинетическую **Ek** и заберется на вершину холма.

Проблема **Mountain car** впервые появилась в докторской диссертации Эндрю Мура в 1990 году. Позже она была описана в других работах, где и была определена более строго. В настоящий момент задача является одной из самых распространенных сред для тестирования работы различных алгоритмов в обучении с подкреплением.

![enter image description here](https://uploads.toptal.io/blog/image/126565/toptal-blog-image-1530883192219-b816bdcbfa175dfa55d23205dd21e9ac.gif)

## ТЕХНИЧЕСКИЕ ДАННЫЕ

### Переменные состояния (States):
В среде используются следующие переменные состояния:

*Скорость(**Velocity**)* = (-0.07, 0.07)

*Позиция(**Position**)* = (-1.2, 0.6)

### Действия (Actions):
В среде используются следующие действия:

*Движение(**Motor**)* = (left, neutral, right)

### Награда (Reward):
В среде предусмотрена награда для каждого очередного шага:

***Reward*** = -1

**Примечание:** В данной работе функция награды специально изменена, т. к. основная цель этой работы -- сравнить скорость обучения различных методов. При этом стоит отметить, что максимальное стандартное количество шагов за одну сессию равно 200.

### Начальное условие (Initial condition):
В среде установлено следующее начальное условие:
***Position*** = $-0.5$
***Velocity*** = $0.0$

## СПОСОБЫ РЕШЕНИЯ
Вообще говоря существует достаточное количество различных способов обучения агента (системы). В данной работе будут представлены два таких способа: **DQN** и **Cross-Entropy**. Они имеют разные реализации, но преследуют одну и ту же цель.

Основная идея проделанной работы заключалась в следующем: Сравнить скорости обучения  методом **DQN** и методом **Cross-Entropy**. Следовательно поэтому в работе специально изменены некоторые параметры, которые, конечно же, отличаются от первичных.

Рассмотрим каждый метод более детально.

## DQN
Сам метод математически можно описать так:

Что по сути, это означает:
Для каждого эпизода делаем три действия:  
 

 1. Находясь в состоянии $S_i$ совершаем действие в силу $eps-greedy$.
    Получаем награду и переходим в следующее состояние   $S_{i+1}$.
    Сохраняем необходимые результаты (четверку) в память  
    
 2. Берем $batch$ из памяти   Определяем целевые значения (см. функцию ACT)   
 Строим функцию потерь (сдвигаем функцию к целевым значениям)
    
 3. Обновляем значение $eps$

Посмотреть на реализацию **DQN** можно в *Приложении 1*

## Cross-Entropy
Вообще говоря, в теории информации **Cross-Entropy** между двумя распределениями вероятностей измеряет среднее число бит, необходимых для опознания события из набора возможностей, если используемая схема кодирования базируется на заданном распределении вероятностей **q**, вместо «истинного» распределения **p**.

В основе данного метода (собственно как и в основах других методов) лежит **Markov Decision Process**

Сам алгоритм выполняет свою работу в неких "**сессиях**" (**sessions**). Кратко описать работу и выявление "**элитных сессий**" (**elite sessions**) можно таким образом:


Посмотреть на реализацию **Cross-Entropy** можно в *Приложении 2*

## АНАЛИЗ РЕЗУЛЬТАТОВ
Ниже приведены результаты работы двух способов --  **DQN** и **Cross-Entropy** соответственно.
Рассмотрим в деталях каждый из методов:

### Анализ DQN
**Примечание:** Для отрисовки более видимого результата было принято решение поощрять агента при достижении "флага" $10000$ очков. Таким образом, мы сможем в увидеть результат обучения в более явном виде.

Было проведено $750$ эпизодов обучения.

Как можно заметить из графика и диаграммы -- **DQN** метод крайне нестабилен. На начальных этапах агент постоянно "проваливается". Затем, поняв методику своего движения - начинает взбираться используя потенциальную энергию (при этом так называемые провалы либо встречаются изредко, либо не встречаются вовсе).  После чего мы можем наблюдать попеременный успех: агент, то "сваливается", то опять начинает достигать цели. Как итог -- метод, действительно, ведет себя нестабильно (можно сказать бинарно: он либо достигает цели много эпизодов подряд, либо же нет).

Проблема данного способа обучения заключается как раз в его нестабильности. Для того, чтобы исправить это необходимо модернизировать **DQN**, например использоваться метод **DDQN** -- **Double DQN**.

### Анализ Cross-Entropy
**Примечание:** Для отрисовки более видимого результата было принято решение поднять суммарное количество наград за сессию на $6000$. Так, мы явно сможем увидеть какие из эпизодов(эпох) содержали элитную сессию, т. е. увидим результат обучения.

Было проведено $30$ эпизодов, с количество сессий $30$ (так мы наглядно сможем увидеть разделение на обычные и элитные сессии) и количеством шагов в каждой сессии равным $6000$.

Наблюдая за графиком и столбчатой диаграммой, можно заметить, что данный метод является более стабильным чем предыдущий. Конечно, у него случаются так называемые "провалы", однако их намного меньше нежели в предыдущем случае. Также видно, как график функции награды растет с увеличением количества эпизодов.

Среди проблем данного метода можно выявить следующее:

 - Крайне нестабильная работа (практически нулевая) на коротких сессиях. А именно короткие сессии длиной в $200$ шагов предусмотрены начальными условиями **Mountain car**. 
 - Выбор функции $\pi$ -- *policy* сильно зависит от разного рода случайности
 - Данный алгоритм работает только с конечными множествами ***States*** и ***Actions***

## ВЫВОД
В результате проделанной работы была решена задача **Mountain car-v0**. При её решении были задействовать методы обучения с подкреплением, а именно: **DQN** и **Cross-Entropy** . Был произведён анализ полученных данных (функции награды), произведено сравнение скоростей обучения данных методов. Помимо этого, были представлены краткие описания обоих принципов и их математическое представление. Были написаны программы на языке Python 3.*, эмулирующие поведение данного агента(системы).

Можно сказать, что изначально поставленная задача была решена.

## Приложения
